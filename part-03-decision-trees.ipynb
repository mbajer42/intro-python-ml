{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are quite popular since they are simple to understand and interpret. There are several algorithms to construct them (e.g. [I3](https://en.wikipedia.org/wiki/ID3_algorithm), [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)), here we will describe [CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29) (Classification And Regression Trees).\n",
    "\n",
    "We will start with regression problems. Our input $x$ consists of $m$ features:\n",
    "\n",
    "$$\n",
    "x = \\left(\n",
    "    \\begin{matrix} \n",
    "    x_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    x_m\n",
    "    \\end{matrix}\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "Note, that unlike in linear- and logistic-regression we don't need the feature $x_0$.\n",
    "A decision tree will split the feature space into a set of rectangles, like in the figure below.\n",
    "\n",
    "![title](img/decision_trees_regions.png)\n",
    "\n",
    "This same model can be represented by a binary tree:\n",
    "\n",
    "![title](img/decision_trees_binary_tree.png)\n",
    "\n",
    "The corresponding model can then predict $y$ with a constant $c_p$:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{k=1}^{5}c_p \\begin{cases} \n",
    "1 & \\text{if } x \\in R_p \\\\\n",
    "0 & \\text{if } x \\notin R_p\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Or in general, if we have already found $P$ partitions:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{k=1}^{P}c_p \\begin{cases} \n",
    "1 & \\text{if } x \\in R_p \\\\\n",
    "0 & \\text{if } x \\notin R_p\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If we use, like in linear regression, the sum of squares ($\\sum(y^{(i)} - f(x^{(i)}))^2$) as our criterion for minimization, the best $c_p$ becomes the average of $y^{(i)}$ in region $R_p$:\n",
    "\n",
    "$$\n",
    "c_p = \\frac{1}{N_p} \\sum_{x^{(i)} \\in R_p} y^{(i)}\n",
    "$$\n",
    "\n",
    "$N_p$ is:\n",
    "\n",
    "$$\n",
    "N_p = \\#\\{x^{(i)} \\in R_p\\}\n",
    "$$\n",
    "\n",
    "To find the best binary partition, characterized by a splitting variable $j$ ($1 \\leq j \\leq m$) and split point s, define a pair of half planes. Where $x_j < s$:\n",
    "\n",
    "$$\n",
    "R_1(j, s) = \\{x | x_j < s\\}\n",
    "$$\n",
    "\n",
    "and $x_j \\geq s$:\n",
    "\n",
    "$$\n",
    "R_2(j, s) = \\{x | x_j \\geq s\\}\n",
    "$$\n",
    "\n",
    "The best splitting variable $j$ and split point $s$ will solve:\n",
    "\n",
    "$$\n",
    "\\min_{j, s} \\Big[\\min_{c_1} \\sum_{x^{(i)} \\in R_1(j, s)} (y^{(i)} - c_1)^2 + \\min_{c_2} \\sum_{x^{(i)} \\in R_2(j, s)} (y^{(i)} - c_2)^2 \\Big]\n",
    "$$\n",
    "\n",
    "As we have already seen, the inner minimization is solved by taking the average of $y^{(i)}$ in the corresponding region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_idx=None,\n",
    "        split_value=None,\n",
    "        predicted_value=None,\n",
    "        left=None,\n",
    "        right=None\n",
    "    ):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.split_value = split_value\n",
    "        self.predicted_value = predicted_value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "def calculate_error(y):\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    return 1 / len(y) * np.sum((y - np.mean(y)) ** 2)\n",
    "\n",
    "\n",
    "def build_tree(X, y):\n",
    "    _, num_features = X.shape\n",
    "        \n",
    "    current_error = calculate_error(y)\n",
    "\n",
    "    best_gain = 0\n",
    "    best_split_feature_idx = None\n",
    "    best_split_feature_val = None\n",
    "    best_split_sets = None\n",
    "        \n",
    "    for feature_idx in range(num_features):\n",
    "        for feature_val in set(X[:, feature_idx]):\n",
    "            \n",
    "            split_condition = X[:, feature_idx] < feature_val\n",
    "            X_left_split, X_right_split = X[split_condition], X[~split_condition]\n",
    "            y_left_split, y_right_split = y[split_condition], y[~split_condition]\n",
    "            \n",
    "            left_split_error = calculate_error(y_left_split)\n",
    "            false_split_error = calculate_error(y_right_split)\n",
    "            error_gain = current_error - left_split_error - false_split_error\n",
    "\n",
    "            if error_gain > best_gain and len(X_left_split) > 0 and len(X_right_split) > 0:\n",
    "                best_gain = error_gain\n",
    "                best_split_feature_idx = feature_idx\n",
    "                best_split_feature_val = feature_val\n",
    "                best_split_sets = ((X_left_split, y_left_split), (X_right_split, y_right_split))\n",
    "                \n",
    "    if best_gain > 0:\n",
    "        left = build_tree(*best_split_sets[0])\n",
    "        right = build_tree(*best_split_sets[1])\n",
    "        \n",
    "        return DecisionTreeNode(\n",
    "            feature_idx=best_split_feature_idx,\n",
    "            split_value=best_split_feature_val,\n",
    "            left=left,\n",
    "            right=right\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        return DecisionTreeNode(\n",
    "            predicted_value=np.mean(y)\n",
    "        )\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._tree = None\n",
    "        \n",
    "    def predict(self, X):\n",
    "        def predict_single_row(node, row):\n",
    "            if node.left is None and node.right is None:\n",
    "                return node.predicted_value\n",
    "            if row[node.feature_idx] < node.split_value:\n",
    "                return predict_single_row(node.left, row)\n",
    "            else:\n",
    "                return predict_single_row(node.right, row)\n",
    "            \n",
    "        return np.asarray([\n",
    "            predict_single_row(self._tree, row) for row in X\n",
    "        ])\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._tree = build_tree(X, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [16]\n",
      " [25]\n",
      " [36]\n",
      " [49]\n",
      " [64]\n",
      " [81]]\n",
      "[ 0.  1.  4.  9. 16. 25. 36. 49. 64. 81.]\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray([i for i in range(10)]).reshape(-1, 1)\n",
    "y = X ** 2\n",
    "y_predicted = DecisionTreeRegressor().fit(X, y).predict(X)\n",
    "\n",
    "print(y)\n",
    "print(y_predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
