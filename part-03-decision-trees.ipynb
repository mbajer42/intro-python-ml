{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are quite popular since they are simple to understand and interpret. There are several algorithms to construct them (e.g. [I3](https://en.wikipedia.org/wiki/ID3_algorithm), [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)), here we will describe [CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29) (Classification And Regression Trees).\n",
    "\n",
    "We will start with regression problems. Our input $x$ consists of $m$ features:\n",
    "\n",
    "$$\n",
    "x = \\left(\n",
    "    \\begin{matrix} \n",
    "    x_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    x_m\n",
    "    \\end{matrix}\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "Note, that unlike in linear- and logistic-regression we don't need the feature $x_0$.\n",
    "A decision tree will split the feature space into a set of rectangles, like in the figure below.\n",
    "\n",
    "![title](img/decision_trees_regions.png)\n",
    "\n",
    "This same model can be represented by a binary tree:\n",
    "\n",
    "![title](img/decision_trees_binary_tree.png)\n",
    "\n",
    "The corresponding model can then predict $y$ with a constant $c_p$:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{k=1}^{5}c_p \\begin{cases} \n",
    "1 & \\text{if } x \\in R_p \\\\\n",
    "0 & \\text{if } x \\notin R_p\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Or in general, if we have already found $P$ partitions:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{k=1}^{P}c_p \\begin{cases} \n",
    "1 & \\text{if } x \\in R_p \\\\\n",
    "0 & \\text{if } x \\notin R_p\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If we use, like in linear regression, the sum of squares ($\\sum(y^{(i)} - f(x^{(i)}))^2$) as our criterion for minimization, the best $c_p$ becomes the average of $y^{(i)}$ in region $R_p$:\n",
    "\n",
    "$$\n",
    "N_p = \\#\\{x^{(i)} \\in R_p\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_p = \\frac{1}{N_p} \\sum_{x^{(i)} \\in R_p} y^{(i)}\n",
    "$$\n",
    "\n",
    "To find the best binary partition, characterized by a splitting variable $j$ ($1 \\leq j \\leq m$) and split point s, define a pair of half planes:\n",
    "\n",
    "$$\n",
    "R_1(j, s) = \\{x | x_j < s\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_2(j, s) = \\{x | x_j \\geq s\\}\n",
    "$$\n",
    "\n",
    "The best splitting variable $j$ and split point $s$ will solve:\n",
    "\n",
    "$$\n",
    "\\min_{j, s} \\Big[\\min_{c_1} \\sum_{x^{(i)} \\in R_1(j, s)} (y^{(i)} - c_1)^2 + \\min_{c_2} \\sum_{x^{(i)} \\in R_2(j, s)} (y^{(i)} - c_2)^2 \\Big]\n",
    "$$\n",
    "\n",
    "As we have already seen, the inner minimization is solved by taking the average of $y^{(i)}$ in the corresponding region."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
